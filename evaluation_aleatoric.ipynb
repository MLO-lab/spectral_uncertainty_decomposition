{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb0682f",
   "metadata": {},
   "source": [
    "# Collect uncertainties and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a87486",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {'dataset': ['AmbigInst', 'AmbigQA'],\n",
    "                    'ensembling_method': ['clarification_zeroshot', 'no_ensembling'],\n",
    "                    'variation_model': ['gpt_4o', 'no_model'],\n",
    "                    'target_model': ['phi_4', 'llama4_maverick'],\n",
    "                    'embedding_model': ['all_mpnet_base_v2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, f1_score\n",
    "\n",
    "def evaluate_uncertainty_predictiveness(uncertainties, labels, eval_measure):\n",
    "    uncertainties = np.array(uncertainties)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # AUC-ROC\n",
    "    auc_roc = roc_auc_score(labels, uncertainties)\n",
    "\n",
    "    # AUC-PR (Average Precision Score)\n",
    "    auc_pr = average_precision_score(labels, uncertainties)\n",
    "\n",
    "    # Best F1 Score\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, uncertainties)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_f1 = np.max(f1_scores)\n",
    "\n",
    "    # Averages\n",
    "    avg_uncertainty_ambiguous = uncertainties[labels == 1].mean() if np.any(labels == 1) else float('nan')\n",
    "    avg_uncertainty_unambiguous = uncertainties[labels == 0].mean() if np.any(labels == 0) else float('nan')\n",
    "\n",
    "    values_dict = {\n",
    "        'AUC_ROC': auc_roc,\n",
    "        'AUC_PR': auc_pr,\n",
    "        'Best_F1': best_f1,\n",
    "        'Avg_uncertainty_amb': avg_uncertainty_ambiguous,\n",
    "        'Avg_uncertainty_unambiguous': avg_uncertainty_unambiguous\n",
    "    }\n",
    "    return values_dict[eval_measure]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_wo_embeddings = {'AUC_ROC': [],\n",
    "        'AUC_PR': [],\n",
    "        'Best_F1': [],\n",
    "        'Avg_uncertainty_amb': [],\n",
    "        'Avg_uncertainty_unambiguous': []}\n",
    "uncertainties_wo_embeddings = []\n",
    "for dataset in hyper_parameters['dataset']:\n",
    "    for ensembling_method in hyper_parameters['ensembling_method']:\n",
    "        for variation_model in hyper_parameters['variation_model']:\n",
    "            for target_model in hyper_parameters['target_model']:\n",
    "                config = {'dataset': dataset,\n",
    "                            'ensembling_method': ensembling_method,\n",
    "                            'variation_model': variation_model,\n",
    "                            'target_model': target_model}\n",
    "                dataset_df = pd.read_json(f\"data/eval/{dataset}.json\")\n",
    "                dataset_df = dataset_df[[\"question_id\", \"is_ambig\"]]\n",
    "                uncertainties_path = os.path.join(f\"data/logs/{dataset}/{ensembling_method}/{variation_model}-variations\",\n",
    "                                                f\"{target_model}-uncertainties_wo_embeddings.json\")\n",
    "                if os.path.exists(uncertainties_path):\n",
    "                    metrics_df = pd.read_json(uncertainties_path)\n",
    "                    metrics = [metric for metric in metrics_df.columns if metric!=\"question_id\"]\n",
    "                    labeled_uncertainties = metrics_df.join(dataset_df, on = \"question_id\", how = \"left\", validate=\"one_to_one\", rsuffix=\"r\")\n",
    "                    for eval_measure in evals_wo_embeddings.keys():\n",
    "                        config_copy = deepcopy(config)\n",
    "                        for metric in metrics:\n",
    "                            config_copy.update({metric: evaluate_uncertainty_predictiveness(labeled_uncertainties[metric], labeled_uncertainties[\"is_ambig\"], eval_measure)})\n",
    "                        evals_wo_embeddings[eval_measure].append(config_copy)\n",
    "                    config_copy = deepcopy(config)\n",
    "                    config_copy.update({'is_ambig': labeled_uncertainties[\"is_ambig\"].to_list()})\n",
    "                    for metric in metrics:\n",
    "                        config_copy.update({metric: labeled_uncertainties[metric].to_list()})\n",
    "                    uncertainties_wo_embeddings.append(config_copy)                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363acb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_w_embeddings = {'AUC_ROC': [],\n",
    "        'AUC_PR': [],\n",
    "        'Best_F1': [],\n",
    "        'Avg_uncertainty_amb': [],\n",
    "        'Avg_uncertainty_unambiguous': []}\n",
    "uncertainties_w_embeddings = []\n",
    "for dataset in hyper_parameters['dataset']:\n",
    "    for ensembling_method in hyper_parameters['ensembling_method']:\n",
    "        for variation_model in hyper_parameters['variation_model']:\n",
    "            for target_model in hyper_parameters['target_model']:\n",
    "                for embedding_model in hyper_parameters['embedding_model']:\n",
    "                    config = {'dataset': dataset,\n",
    "                                'ensembling_method': ensembling_method,\n",
    "                                'variation_model': variation_model,\n",
    "                                'target_model': target_model,\n",
    "                                'embedding_model': embedding_model}\n",
    "                    dataset_df = pd.read_json(f\"data/eval/{dataset}.json\")\n",
    "                    dataset_df = dataset_df[[\"question_id\", \"is_ambig\"]]\n",
    "                    uncertainties_path = os.path.join(f\"data/logs/{dataset}/{ensembling_method}/{variation_model}-variations\",\n",
    "                                                    f\"{target_model}-uncertainties_w_embeddings-{embedding_model}.json\")\n",
    "                    if os.path.exists(uncertainties_path):\n",
    "                        metrics_df = pd.read_json(uncertainties_path)\n",
    "                        metrics = [metric for metric in metrics_df.columns if metric!=\"question_id\"]\n",
    "                        labeled_uncertainties = metrics_df.join(dataset_df, on = \"question_id\", how = \"left\", validate=\"one_to_one\", rsuffix=\"r\")\n",
    "                        for eval_measure in evals_w_embeddings.keys():\n",
    "                            config_copy = deepcopy(config)\n",
    "                            for metric in metrics:\n",
    "                                config_copy.update({metric: evaluate_uncertainty_predictiveness(labeled_uncertainties[metric], labeled_uncertainties[\"is_ambig\"], eval_measure)})\n",
    "                            evals_w_embeddings[eval_measure].append(config_copy)\n",
    "                        config_copy = deepcopy(config)\n",
    "                        config_copy.update({'is_ambig': labeled_uncertainties[\"is_ambig\"].to_list()})\n",
    "                        for metric in metrics:\n",
    "                            config_copy.update({metric: labeled_uncertainties[metric].to_list()})\n",
    "                        uncertainties_w_embeddings.append(config_copy)                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_wo_embeddings_df = {key: pd.DataFrame(value) for key, value in evals_wo_embeddings.items()}\n",
    "evals_w_embeddings_df = {key: pd.DataFrame(value) for key, value in evals_w_embeddings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f65d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_wo_embeddings_df['AUC_ROC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20042a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_w_embeddings_df['AUC_ROC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30effe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainties_w_embeddings_df = pd.DataFrame(uncertainties_w_embeddings)\n",
    "uncertainties_wo_embeddings_df = pd.DataFrame(uncertainties_wo_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainties_w_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainties_wo_embeddings_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation(evaluation_df, query_dict, uncertainty_metric, formula_part):\n",
    "    mask = np.logical_and.reduce([evaluation_df[k] == v for k, v in query_dict.items()])\n",
    "    filtered_df = evaluation_df[mask]\n",
    "    assert len(filtered_df) == 1, f\"{len(filtered_df)}, {query_dict}\"\n",
    "    full_metric_name = f\"{uncertainty_metric}_{formula_part}\"\n",
    "    return filtered_df.iloc[0][full_metric_name]\n",
    "\n",
    "def get_labels(uncertainties_df, query_dict):\n",
    "    mask = np.logical_and.reduce([uncertainties_df[k] == v for k, v in query_dict.items()])\n",
    "    filtered_df = uncertainties_df[mask]\n",
    "    assert len(filtered_df) == 1, f\"{len(filtered_df)}, {query_dict}\"\n",
    "    return filtered_df.iloc[0]['is_ambig']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e0967",
   "metadata": {},
   "source": [
    "# Main results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_main_table(evals_w_embeddings_df, evals_wo_embeddings_df, dataset, target_model, embedding_model):\n",
    "    eval_metric_names = ['AUC_ROC', 'AUC_PR']\n",
    "    \n",
    "    uncertainty_method_to_metric = {'semantic entropy': 'discrete_semantic_entropy',\n",
    "                                    'kernel language entropy': 'kernel_language_entropy',\n",
    "                                    'predictive kernel entropy': 'predictive_kernel_entropy',\n",
    "                                    'input clarification ensembling': 'discrete_semantic_entropy',\n",
    "                                    'spectral uncertainty': 'von_neuman_entropy'}\n",
    "    \n",
    "    \n",
    "\n",
    "    uncertainty_method_to_ensembling_method = {'semantic entropy': 'no_ensembling',\n",
    "                                    'kernel language entropy': 'no_ensembling',\n",
    "                                    'predictive kernel entropy': 'no_ensembling',\n",
    "                                    'input clarification ensembling': 'clarification_zeroshot',\n",
    "                                    'spectral uncertainty': 'clarification_zeroshot'}\n",
    "    uncertainty_method_to_formula_part = {'semantic entropy': 'total',\n",
    "                                    'kernel language entropy': 'total',\n",
    "                                    'predictive kernel entropy': 'total',\n",
    "                                    'input clarification ensembling': 'disagreement',\n",
    "                                    'spectral uncertainty': 'disagreement'}\n",
    "    uncertainty_method_to_variation_model = {'semantic entropy': 'no_model',\n",
    "                                    'kernel language entropy': 'no_model',\n",
    "                                    'predictive kernel entropy': 'no_model',\n",
    "                                    'input clarification ensembling': 'gpt_4o',\n",
    "                                    'spectral uncertainty': 'gpt_4o'}\n",
    "    uncertainty_method_to_target_model = {'semantic entropy': target_model,\n",
    "                                    'kernel language entropy': target_model,\n",
    "                                    'predictive kernel entropy': target_model,\n",
    "                                    'input clarification ensembling': target_model,\n",
    "                                    'spectral uncertainty': target_model}\n",
    "    \n",
    "\n",
    "\n",
    "    uncertainty_method_to_df = {'semantic entropy': evals_wo_embeddings_df,\n",
    "                                    'kernel language entropy': evals_wo_embeddings_df,\n",
    "                                    'predictive kernel entropy': evals_w_embeddings_df,\n",
    "                                    'input clarification ensembling': evals_wo_embeddings_df,\n",
    "                                    'spectral uncertainty': evals_w_embeddings_df}\n",
    "    \n",
    "    \n",
    "    \n",
    "    table_lines = []\n",
    "    for method in uncertainty_method_to_metric.keys(): #Methods without embeddings\n",
    "        if method not in ['spectral uncertainty', 'predictive kernel entropy']:\n",
    "            query_dict = {'dataset': dataset,\n",
    "                          'target_model': uncertainty_method_to_target_model[method],\n",
    "                          'ensembling_method': uncertainty_method_to_ensembling_method[method],\n",
    "                          'variation_model': uncertainty_method_to_variation_model[method]}\n",
    "            method_dict = {'Uncertainty Method': method}\n",
    "            for eval_metric in eval_metric_names:\n",
    "                method_dict[eval_metric] = get_evaluation(uncertainty_method_to_df[method][eval_metric], query_dict, uncertainty_method_to_metric[method], uncertainty_method_to_formula_part[method])\n",
    "            table_lines.append(method_dict)\n",
    "    for method in ['predictive kernel entropy', 'spectral uncertainty']: #Methods with embeddings\n",
    "        query_dict = {'dataset': dataset,\n",
    "                        'target_model': uncertainty_method_to_target_model[method],\n",
    "                        'ensembling_method': uncertainty_method_to_ensembling_method[method],\n",
    "                        'variation_model': uncertainty_method_to_variation_model[method],\n",
    "                        'embedding_model': embedding_model}\n",
    "        method_dict = {'Uncertainty Method': method }\n",
    "        for eval_metric in eval_metric_names:\n",
    "            method_dict[eval_metric] = get_evaluation(uncertainty_method_to_df[method][eval_metric], query_dict, uncertainty_method_to_metric[method], uncertainty_method_to_formula_part[method])\n",
    "        table_lines.append(method_dict)\n",
    "\n",
    "    return pd.DataFrame(table_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a735f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_main_table(evals_w_embeddings_df, evals_wo_embeddings_df, 'AmbigQA', 'phi_4', 'all_mpnet_base_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85533b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_main_table(evals_w_embeddings_df, evals_wo_embeddings_df, 'AmbigInst', 'phi_4', 'all_mpnet_base_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_main_table(evals_w_embeddings_df, evals_wo_embeddings_df, 'AmbigQA', 'llama4_maverick', 'all_mpnet_base_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69680acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_main_table(evals_w_embeddings_df, evals_wo_embeddings_df, 'AmbigInst', 'llama4_maverick', 'all_mpnet_base_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb658d",
   "metadata": {},
   "source": [
    "# KDE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_kde_data(uncertainties_w_embeddings_df, uncertainties_wo_embeddings_df, dataset, target_model, embedding_model):    \n",
    "    uncertainty_method_to_metric = {'Semantic Entropy': 'discrete_semantic_entropy',\n",
    "                                    'Kernel Language Entropy': 'kernel_language_entropy',\n",
    "                                    'Predictive Kernel Entropy': 'predictive_kernel_entropy',\n",
    "                                    'Input Clarification Ensembling (aleatoric)': 'discrete_semantic_entropy',\n",
    "                                    'Spectral Uncertainty (aleatoric)': 'von_neuman_entropy'}\n",
    "    \n",
    "    uncertainty_method_to_ensembling_method = {'Semantic Entropy': 'no_ensembling',\n",
    "                                    'Kernel Language Entropy': 'no_ensembling',\n",
    "                                    'Predictive Kernel Entropy': 'no_ensembling',\n",
    "                                    'Input Clarification Ensembling (aleatoric)': 'clarification_zeroshot',\n",
    "                                    'Spectral Uncertainty (aleatoric)': 'clarification_zeroshot'}\n",
    "    uncertainty_method_to_formula_part = {'Semantic Entropy': 'total',\n",
    "                                    'Kernel Language Entropy': 'total',\n",
    "                                    'Predictive Kernel Entropy': 'total',\n",
    "                                    'Input Clarification Ensembling (aleatoric)': 'disagreement',\n",
    "                                    'Spectral Uncertainty (aleatoric)': 'disagreement'}\n",
    "    uncertainty_method_to_variation_model = {'Semantic Entropy': 'no_model',\n",
    "                                    'Kernel Language Entropy': 'no_model',\n",
    "                                    'Predictive Kernel Entropy': 'no_model',\n",
    "                                    'Input Clarification Ensembling (aleatoric)': 'gpt_4o',\n",
    "                                    'Spectral Uncertainty (aleatoric)': 'gpt_4o'}\n",
    "    uncertainty_method_to_target_model = {'Semantic Entropy': target_model,\n",
    "                                    'Kernel Language Entropy': target_model,\n",
    "                                    'Predictive Kernel Entropy': target_model,\n",
    "                                    'Input Clarification Ensembling (aleatoric)': target_model,\n",
    "                                    'Spectral Uncertainty (aleatoric)': target_model}\n",
    "    \n",
    "\n",
    "\n",
    "    uncertainty_method_to_df = {'Semantic Entropy': uncertainties_wo_embeddings_df,\n",
    "                                    'Kernel Language Entropy': uncertainties_wo_embeddings_df,\n",
    "                                    'Predictive Kernel Entropy': uncertainties_w_embeddings_df,\n",
    "                                    'Input Clarification Ensembling (aleatoric)': uncertainties_wo_embeddings_df,\n",
    "                                    'Spectral Uncertainty (aleatoric)': uncertainties_w_embeddings_df}\n",
    "    uncertainty_method_to_embedding_model = {'Semantic Entropy': None,\n",
    "                                    'Kernel Language Entropy': None,\n",
    "                                    'Predictive Kernel Entropy': embedding_model,\n",
    "                                    'Input Clarification Ensembling (aleatoric)': None,\n",
    "                                    'Spectral Uncertainty (aleatoric)': embedding_model}\n",
    "    \n",
    "    \n",
    "    method_names = []\n",
    "    method_uncertainties = []\n",
    "    for method in uncertainty_method_to_metric.keys(): \n",
    "        query_dict = {'dataset': dataset,\n",
    "                        'target_model': uncertainty_method_to_target_model[method],\n",
    "                        'ensembling_method': uncertainty_method_to_ensembling_method[method],\n",
    "                        'variation_model': uncertainty_method_to_variation_model[method]}\n",
    "        if uncertainty_method_to_embedding_model[method] is not None:\n",
    "            query_dict['embedding_model'] = uncertainty_method_to_embedding_model[method]\n",
    "        method_names.append(method)\n",
    "        method_uncertainties.append(get_evaluation(uncertainty_method_to_df[method], query_dict, uncertainty_method_to_metric[method], uncertainty_method_to_formula_part[method]))\n",
    "        is_ambig = get_labels(uncertainty_method_to_df[method], query_dict)\n",
    "\n",
    "    return is_ambig, method_uncertainties, method_names\n",
    "\n",
    "\n",
    "def plot_uncertainty_distributions(is_ambig, uncertainty_methods, method_names):\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 10,\n",
    "        'axes.titlesize': 12,\n",
    "        'axes.labelsize': 10,\n",
    "        'legend.fontsize': 9,\n",
    "        'xtick.labelsize': 9,\n",
    "        'ytick.labelsize': 9,\n",
    "        'axes.edgecolor': 'black',\n",
    "        'axes.linewidth': 0.8,\n",
    "        'grid.color': '#e0e0e0',\n",
    "        'grid.linestyle': '--',\n",
    "    })\n",
    "    n_methods = len(uncertainty_methods)\n",
    "\n",
    "        \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8)) #todo change to 3x2 grid, 8x12 size\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (method_uncertainties, name) in enumerate(zip(uncertainty_methods, method_names)):\n",
    "        ax = axes[i]\n",
    "        ambig_values = [u for u, a in zip(method_uncertainties, is_ambig) if a]\n",
    "        nonambig_values = [u for u, a in zip(method_uncertainties, is_ambig) if not a]\n",
    "        \n",
    "        sns.kdeplot(ambig_values, ax=ax, label='Ambiguous', fill=True, alpha = 0.4, linewidth=1.5, color='#E24A33')\n",
    "        sns.kdeplot(nonambig_values, ax=ax, label='Non-ambiguous', fill=True, alpha = 0.4, linewidth=1.5, color='#348ABD')\n",
    "        \n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel('Uncertainty')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "\n",
    "    # Hide any unused subplots if < 6 methods\n",
    "    for j in range(n_methods, 6):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc88263",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ambig, method_uncertainties, method_names = generate_kde_data(uncertainties_w_embeddings_df, uncertainties_wo_embeddings_df, 'AmbigQA', 'phi_4', 'all_mpnet_base_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_uncertainty_distributions(is_ambig, method_uncertainties, method_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4197ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ambig, method_uncertainties, method_names = generate_kde_data(uncertainties_w_embeddings_df, uncertainties_wo_embeddings_df, 'AmbigInst', 'phi_4', 'all_mpnet_base_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_uncertainty_distributions(is_ambig, method_uncertainties, method_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53305ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ambig, method_uncertainties, method_names = generate_kde_data(uncertainties_w_embeddings_df, uncertainties_wo_embeddings_df, 'AmbigQA', 'llama4_maverick', 'all_mpnet_base_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b873c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_uncertainty_distributions(is_ambig, method_uncertainties, method_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d434da",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ambig, method_uncertainties, method_names = generate_kde_data(uncertainties_w_embeddings_df, uncertainties_wo_embeddings_df, 'AmbigInst', 'llama4_maverick', 'all_mpnet_base_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_uncertainty_distributions(is_ambig, method_uncertainties, method_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc330733",
   "metadata": {},
   "source": [
    "# Pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb415a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance_histograms(dataset, model):\n",
    "    with open(f'data/logs/{dataset}/clarification_zeroshot/gpt_4o-variations/{model}-embeddings-all_mpnet_base_v2.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    input_df = pd.read_json(f'data/logs/{dataset}/clarification_zeroshot/gpt_4o-variations/{model}-answers.json')\n",
    "    input_df['embedding'] = embeddings\n",
    "    input_df = input_df[['question_id', 'variant_id', 'answer_id', 'embedding']]\n",
    "    question_ids = np.unique(input_df[\"question_id\"]) #Sorted unique values\n",
    "    distances = []\n",
    "    for question_id in tqdm(question_ids):\n",
    "        question_df = input_df[input_df[\"question_id\"] == question_id]\n",
    "        question_df = question_df.reset_index(drop = True)\n",
    "        for i, row in question_df.iterrows():\n",
    "            for j in range(i+1, len(question_df)):\n",
    "                other_row = question_df.iloc[j]\n",
    "                if row['variant_id']!= other_row['variant_id']:\n",
    "                    distance = np.linalg.norm(np.array(row['embedding']) - np.array(other_row['embedding']))\n",
    "                    distances.append(distance)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(\n",
    "        distances,\n",
    "        bw_adjust=0.2,     \n",
    "        cumulative=True,    # Enable CDF\n",
    "        linewidth=2,\n",
    "        color='#E24A33' if dataset == \"AmbigInst\" else '#348ABD',    # Orange-red (colorblind-friendly)\n",
    "        label='Pairwise Distance CDF',\n",
    "        fill=True,\n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.xlabel('Distance', fontsize=12)\n",
    "    plt.xlim(-0.05,1.6)\n",
    "    plt.ylabel('Cumulative Probability', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=0.4)\n",
    "    plt.legend(frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd001af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distance_histograms('AmbigQA', 'phi_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distance_histograms('AmbigInst', 'phi_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distance_histograms('AmbigQA', 'llama4_maverick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distance_histograms('AmbigInst', 'llama4_maverick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a3d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75a91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
